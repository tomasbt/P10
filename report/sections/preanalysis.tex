\chapter{Application Analysis} \label{ch:appanalysis}
This chapter will explore the application domain from the A$^3$ model. First, the basic principles of stereo vision will be described and then other aspects such as color versus grayscale etc. are analyzed. The results from this chapter will be used in the next chapter \vref{ch:req}.

\section{The Basic Principal of Stereo Vision}\label{sec:basicstereo}
A standard stereo vision setup consists of two similar cameras placed horizontally at a specified distance from each other. This distance is called the baseline. Figure \vref{fig:2cams_all} shows an example of this setup.\\
\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{1\textwidth}
    \centering\includegraphics[width=0.4\textwidth]{figures/2cams_fro}
    \caption{Seen from the front\label{fig:2cams_fro}}
  \end{subfigure}\vspace{0.4cm}
  \begin{subfigure}[t]{1\textwidth}
    \centering\includegraphics[width=0.4\textwidth]{figures/2cams_top}
    \caption{Seen from above\label{fig:2cams_top}}
  \end{subfigure}
  \caption{Illustration of a standard stereo setup\label{fig:2cams_all}}
\end{figure}

Figure \vref{fig:imgplane_all} shows how a scene is seen by the camera, is inverted in the optical center and projected onto the image sensor in the camera. The original image plane is located at the position of the image sensor but it is inverted compared to the scene captured. To simplified comparisons to the real world, an image plane can be placed opposite of the optical center at the same distance from the center and this image plane will not be inverted.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[t]{0.3\textwidth}
    \centering\includegraphics[height=4cm]{figures/imgplane_1.jpg}
    \caption{Location of the optical center and image sensor\label{fig:imgplane1}}
  \end{subfigure}\hspace{0.5cm}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering\includegraphics[height=4cm]{figures/imgplane_2}
    \caption{Location of the image plane\label{fig:imgplane2}}
  \end{subfigure}\hspace{0.5cm}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering\includegraphics[height=4cm]{figures/imgplane_3}
    \caption{This illustration will be used to to explain disparity\label{fig:imgplane3}}
  \end{subfigure}
  \begin{subfigure}[t]{0.75\textwidth}
    \centering\includegraphics[width=0.4\textwidth]{figures/imgplane_legends}
  \end{subfigure}\hspace{0.5cm}
  \caption{Illustration of going from camera to image plane\label{fig:imgplane_all}}
\end{figure}

Figure \vref{fig:2points_1} shows that a single camera is not able to differentiate between two points at the same angle from the optical center but a different distance. Figure \vref{fig:2points_2} shows that adding the second camera shows that you then are able to differentiate between the two points.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering\includegraphics[height=4cm]{figures/2points_1}
    \caption{Seen from a single camera\label{fig:2points_1}}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering\includegraphics[height=4cm]{figures/2points_2}
    \caption{Seen from two cameras\label{fig:2points_2}}
  \end{subfigure}
  \caption{Example pf two points in a scene at different depths\label{fig:2points_all}}
\end{figure}

Figure \vref{fig:dispall} shows how the distance to a point can be calculated from the difference in x-positions on the image plane (the disparity). Figure \vref{fig:disp_long} and \vref{fig:disp_short} shows how the disparity change depending on the distance to the point. Figure \vref{fig:bfz_disp} shows the point in the scene (p), where line of sight cross the image planes (i$_1$ and i$_2$) and the optical centers (c$_1$ and c$_2$). From these points two similarly angled  triangles can be created. One between p, i$_1$ and i$_2$ and the other triangle between p, c$_1$ and c$_2$. For similarly angled triangles the ratio between the height and the bottom width is the same for each triangle and hence the following equation can be formed:
\begin{flalign}
  && \frac{b}{z} &= \frac{L}{z-f} = \frac{b-(x_1-x_2)}{z-f} && \label{eq:disp_1}
\end{flalign}
$x_1-x_2$ is also called the disparity, $d$, and equation \vref{eq:disp_1} can be simplified:
\begin{flalign}
  && &z = \frac{b \cdot f}{d} && \label{eq:disp_final}
\end{flalign}
In equation \vref{eq:disp_final} $b$ and $f$ is known hence only the disparity is needed to find $z$. So to find the distance to a point you should find the location of the point in the two images and calculate the displacement. 

\begin{figure}[ht]
  \centering
  \begin{subfigure}[t]{0.3\textwidth}
    \centering\includegraphics[height=7cm]{figures/disp_long.jpg}
    \caption{Disparity for a point far way\label{fig:disp_long}}
  \end{subfigure}\hspace{0.5cm}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering\includegraphics[height=7cm]{figures/disp_short}
    \caption{Disparity for point close\label{fig:disp_short}}
  \end{subfigure}\hspace{0.5cm}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering\includegraphics[height=7cm]{figures/bfz_disp}
    \caption{Illustration of triangles used for calculating the disparity\label{fig:bfz_disp}}
  \end{subfigure}
  \caption{Illustration of how to calculate depth from disparity\label{fig:dispall}}
\end{figure}

This explains the basic of stereo vision. The rest of this chapter will venture into other areas of stereo and describe the difficulties and solutions for each area.

\section{Epipolar Geometry}
Section \ref{sec:basicstereo} assumes that the stereo image planes are ideal, align exactly and being parallel with the baseline but in a real scenario the cameras will have small imperfections and variations which will make the image planes not align perfectly.\\
 
Figure \vref{fig:nonrect} shows a pair of stereo images. As seen when searching for a corresponding point in the second camera (e.g. the top of the bottle) then a 2D search area is needed. To simplify the search epipolar geometry can be used. \\
\begin{figure}[ht!]
  \centering\includegraphics[height=5cm]{figures/nonrect.jpg}
  \caption{Non-rectified stereo pair\label{fig:nonrect} \cite{Mattoccia2013}}
\end{figure}

Epipolar geometry occurs when a scene is seen from two different views. Figure \vref{fig:epipolarsimple} illustrates epipolar geometry. An epipole is the projection, on one cameras image plane, of the optical center of the other camera and is illustrated on figure \vref{fig:epipolarsimple} as e$_1$ and e$_2$. The red line going through p$_1$ (the projection of point P on the image plane) and e$_1$ is called an epipolar line and a corresponding epipolar line can be found on the other image plane going through p$_2$ and e$_2$. When searching for the corresponding point in the other image the search can be simplified from a 2D search to a 1D search along the epipolar. To simplify the search further, the image can be rectified.\\
\begin{figure}[ht!]
  \centering
  \includegraphics[height=4cm]{figures/epipolarsimple}
  \caption{Illustration of epipolar geometry}
  \label{fig:epipolarsimple}
\end{figure}

Rectification will transform the stereo images to remove lens distortion and make them into standard form. The standard form is helpful since all epipolar lines then will be horizontal and this simplifies the search for corresponding points to search along the x-axis. Figure \vref{fig:rect} shows the stereo image pair from figure \vref{fig:nonrect} but rectified. As seen, now the corresponding points can be found by following the horizontal lines or the x-axis.\\

\begin{figure}[ht!]
  \centering
  \includegraphics[height=4cm]{figures/rect}
  \caption{Rectified stereo pair\label{fig:rect} \cite{Mattoccia2013}}
\end{figure}

The issue with rectification is that it is difficult to get a perfect match with the stereo setup since the every camera and its objective will be unique and require and all new manual adjustment. HSA system theorizes that a system can be developed which instead of rectifying the image it will find the epipolar lines and feed this information to the stereo cameras.\\

In this project stereo image pairs from Middlebury Vision Test sets \cite{middlebury2016} will be used which are rectified hence the rectification of image will not be researched further in this project.

\section{Color space and grayscale}
Colors can be represented in many different ways digitally using color spaces. Two of the most common color spaces are grayscale and RGB. Some articles have researched what impact different color spaces can have on the result from a stereo algorithm.\\ 

For this project the impact of color spaces haven't been studied but instead the findings in \cite{chambon2005colour} are used. This article studies the impact of color spaces on stereo matching by investigating 9 color spaces and 3 different methods. Table \vref{tab:colourres} shows parts of the results from \cite{chambon2005colour}. In this table the \textit{Measure} column is the algorithm used, \textit{Type} specifies whether it is grayscale (G) or color (C) with the best color space used, the \textit{Correct} column is the percentage of correct matches and \textit{Time} is the execution time. The article concludes that using color always results in better matching but from table \vref{tab:colourres} it is noticed that using grayscale lowers the execution time a lot (down to 27-51 \% of color execution time) and in most cases not resulting much worse matching results. \\
\begin{table}
  \centering
  \begin{tabular}{l r | c | c }
    Measure & Type & Correct [\%] & Time\\
    \midrule
    Ncc & G & 52.3 & 52\\
          & C & 55.2 & 141\\
    \midrule
    D$_1$ & G & 49.5 & 63\\
               & C & 51.6 & 140\\
    \midrule
    PRATT & G & 29.1 & 86\\
              & C & 45.2 & 225\\
    \midrule
    ISC & G & 44.9 & 126\\
    & C & 52.6 & 245\\
    \midrule
    SMPD$_2$ & G & 49.9 & 569\\
    & C & 56.5 & 2109 \\
  \end{tabular}
  \caption{Part of table containing results from \cite{chambon2005colour}\label{tab:colourres}}
\end{table}

Since the project mostly focuses on a fast stereo matching algorithm and minding the results from table \vref{tab:colourres} it is decided to use grayscale images in case Normalized Cross Correlation is used or to check whether grayscale images gives a much worse result of another algorithm.

\section{Disparity precision}\label{sec:disppre}
The depth resolution depends on different things in the stereo camera setup. The camera resolution, the focal length, the baseline etc. HSA system requires that the system have a \SI{\leq 2}{\milli\meter} depth resolution between \SI{0.5}{\meter} and \SI{1.5}{\meter}.\\
For a camera HSA systems have a \textit{Imaging Source DMK 72BUC02} available for a stereo camera prototype and the specifications from this camera will be used for the calculations in this section. The camera has a resolution of 2592$\times$1944 and a pixel size of \SI{2.2}{\micro\meter}. It should also be noted that this camera is monochrome and only have a frame rate of 6 fps hence it cannot be used for a final prototype but these specifications are still used since the camera is available at HSA system and The Imaging Source have similar cameras with higher frame rate and with color. More specifications for the camera can be found at \cite{imagingsource2016}. A baseline of 125 mm is chosen since HSA system has a simple stereo camera prototype with a baseline of 125 mm.\\
To calculate the precision equation \vref{eq:disp_final} is used. This equation is repeated here:
\begin{flalign}
 && z &= \frac{b\cdot f}{d} && \label{eq:disp_final2}
\end{flalign}
An issue with \vref{eq:disp_final2} is that the disparity, $d$, is expressed in pixels but to convert it to mm the pixel size of the camera can be used. The lens of the camera can be changed and HSA system has some different lenses with different focal lengths and these are: 6, 9, 16, 25, and 50 mm. Figure \vref{fig:dispPreFocal} shows a plot of disparity precision at around 1500 mm distance from the cameras using the camera mentioned and a baseline of 125 mm. The red dots shows the result for the focal length available at HSA system. The green line and red line shows when 4 mm precision and 2 mm precision is achieved. The reason for 4 mm precision is shown is because this can be improved to 2 mm precision using subpixel refinement.\\

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\textwidth]{figures/dispPreFocal}
  \caption{Disparity precision at  $z \approx 1500$ mm compared to focal length with baseline = 125 mm and using \textit{Imaging Source DMK 72BUC02}}
  \label{fig:dispPreFocal}
\end{figure}

To calculate the disparity precision the disparity value for a distance, $z$, of at least 1500 mm should be found. Equation \vref{fig:disp_final2} can be rearranged and pixel size, $p_{size}$, can be added to calculate a disparity value for a specific distance:
\begin{flalign}
  && d &= \frac{b \cdot f}{p_{size}\cdot z} && \label{eq:dispatz}
\end{flalign}
The disparity value found by equation \vref{eq:dispatz} might not be a integer which the disparity always will be hence the value should be rounded down to ensure an valid value which results in a distance of at least 1500 mm. Adding the known values the following equation is acquired:
\begin{flalign}
  && d_{1500mm} &= \left\lfloor \frac{125 \cdot f}{0.0022 \cdot 1500} \right\rfloor && 
\end{flalign}
Then precision at $\approx 1500$ mm can be found by using equation \vref{eq:disp_final2} with disparity, $d_{1500mm}$, and subtract the distance for disparity, $d_{1500mm} + 1$: 
\begin{flalign}
  && d_p &= z_{d_{1500mm}} - z_{d_{1500mm}+1} && \\
  && d_p &= \frac{b \cdot f}{p_{size}\cdot d_{1500mm}} - \frac{b \cdot f}{p_{size}\cdot (d_{1500mm}+1)} && \label{eq:dispPre}
\end{flalign}
Equation \vref{eq:dispPre} have been used for the plot in figure \vref{fig:dispPreFocal}. From the figure it is noticed that 4 mm precision achieved at a focal length of $\approx 10$ mm and 2 mm precision achieved at a focal length of $\approx 20$ mm. From this a higher focal length is wanted but a higher focal length have a bad influence on scene width and disparity range.\\
Figure \vref{fig:scewiwall} shows how focal length influences the scene width. It is noticed that a higher focal length results in a smaller scene hence it is wanted to have a small focal length.\\

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering\includegraphics[scale=0.75]{figures/scewiw.jpg}
    \caption{Illustration of how a short focal length affects the scene width\label{fig:scewiw}}
  \end{subfigure}\hspace{0.5cm}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering\includegraphics[scale=0.75]{figures/scewiw2}
    \caption{Illustration of how a long focal length affects the scene width\label{fig:scewiw2}}
  \end{subfigure}
  \begin{subfigure}[t]{1\textwidth}
    \centering\includegraphics[width=1\textwidth]{figures/focalSceWid}
    \caption{Scene width at z = 1500 mm with different focal length while the other specfication are constant\label{fig:scewfocalplot}}
  \end{subfigure}
  \caption{Illustration of how focal length affects the scene width at z = 1500 mm \label{fig:scewiwall}}
\end{figure}
Figure \vref{fig:drange} shows the disparity value at maximum distance ($z\approx 1500$ mm), the disparity value at minimum distance ($z \approx 500$ mm) and the range between these two values. As seen the disparity range increases linear with the focal length which results in a large search area hence a lower focal length is wanted.\\
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\textwidth]{figures/drange}
  \caption{Graph of disparity values at minimum distance and maximum distance and the disparity range}
  \label{fig:drange}
\end{figure}
Using a camera with the same pixel size and resolution as \textit{Imaging Source DMK 72BUC02} a lens with a focal length of 10 mm is wanted while also using subpixel refinement or a lens with a focal length of 20 mm is wanted if subpixel refinement is not being used. \\
A 10 mm focal length will result in a scene width of 855 mm, a disparity range of 759 and a precision of 3.97 mm. A 20 mm focal length will result in a scene width of 428 mm, a disparity range of 1516 and a precision of 1.98 mm.\\
Since a disparity range of 1516 is high and a scene width of 428 mm is very small then a 10 mm lens should use together with subpixel refinement.\\

\section{Occlusions}
Occlusions occur when an object closer to the camera setup blocks some of an object or a whole object behind it. Figure \ref{fig:occlboth} illustrates two cylinders seen by a stereo camera setup where occlusion occurs. Figure \vref{fig:occltop} shows that the right camera can't see all of the blue cylinder because the red cylinder blocks the view while the left camera can see the whole of both cylinder. When searching for corresponding points in the occluded area issues occurs. Following the arrow on figure \vref{fig:occl2view} it is seen that the edge of the blue cylinder can't be found and it will result in calculating a wrong disparity value. \\

\begin{figure}[ht]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering\includegraphics[scale=0.4]{figures/occltop.jpg}
    \caption{Stereo camera setup with two cylinders in the scene seen from above\label{fig:occltop}}
  \end{subfigure}\hspace{0.5cm}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering\includegraphics[scale=0.4]{figures/occl2view}
    \caption{Image seen by each camera in figure \vref{fig:occltop}\label{fig:occl2view}}
  \end{subfigure}
  \caption{Illstration of occlusions\label{fig:occlboth}}
\end{figure}

There exist different types of occlusions: partial occlusions, self-occlusions, border occlusions and total occlusions. Partial occlusions are when an object is only partial obstructed as seen on figure \vref{occlboth}. Self-occlusion occurs on round surfaces such as faces, and balls and as seen on figure \vref{fig:occlself} the surfaces marked with red can be seen by one camera but not the other camera. Border occlusions occur when an object or part of an object is outside the view of one camera but not the other camera as seen with the blue object on figure \ref{fig:occltb}. Total occlusion is when an object is completely blocked by an object in the view of one camera but not in the view of the other camera and an example of this is seen with the red object on figure \vref{fig:occltb}.\\

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering\includegraphics[scale=0.4]{figures/occlself.jpg}
    \caption{Illustartion of self occlusion seen from above\label{fig:occlself}}
  \end{subfigure}\hspace{0.5cm}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering\includegraphics[scale=0.4]{figures/occltotalborder}
    \caption{Illustration of total and border occlusions seen from above\label{fig:occltb}}
  \end{subfigure}
  \caption{Illustration of different types of occlusions\label{fig:occltypes}}
\end{figure}

Occluded points can be found by running stereo matching again but switching which camera is used as the reference and then compare the disparity values found for each direction. When occluded areas are found these can be filled using different methods. For this project, new methods haven't been developed or studied but instead the findings from \cite{huq2013occlusion} is used which will be described in section \vrefrange{sec:nda}{sec:sls}. All these methods assume that the stereo matching is using the left image as reference i.e. the points in the left image are searched for in the right image. 

\subsection{Neighbor's Disparity Assignment (NDA)}\label{sec:nda}
This is one of the simpler methods to fill occlusions. It functions by selecting an occluded point, $p_L$, then finds then nearest non-occluded point, $q_L$, to the left when filling non-border occlusion. With border occlusion, the nearest point to the right is found instead. This method assumes that this non-occluded point is part of the same surface as the occluded point (this can be seen on figure \ref{fig:occlboth}) and the disparity value from $q_L$ can be assigned to $p_L$. This method has some issues. In cases of total occlusions (see figure \ref{fig:occltb}) wrong disparity values will be given to the total occluded object since it isn't a part of the nearest surface with non-occluded points to the left. In cases with self-occlusions the occluded area should have disparity values close to the disparity values of the non-occluded points to the right (This will be the area of the surface which is in view of both cameras) but using NDA will give the occluded area disparity values corresponding to the background.

\subsection{Diffusion in Intensity Space (DIS)}
This method is inspired by diffusion. Diffusion is the movement of molecules or atoms from a high concentration region to a low concentration region. \\

After detecting occluded regions with cross-checking during stereo matching, the diffusion energy for the region is approximated. This method is depended on the stereo matching algorithm because it use the energy from the last iteration to determine initial diffusion energy for the area. But a change to the method can be made to make it independent from the stereo matching. The initial energy will be 0. Then the diffusion energy for non-border occlusion is found by:
\begin{equation}
E(p_L) = \min_{l_{p_L}=\{0,\dots, l_{max}\}} \left( \dfrac{1}{2 | q_L \in \mathcal{N}(p_L) \wedge l_{q_L=l_{p_L}} |} \; \sum_{q_L \in \mathcal{N}(p_L) \wedge l_{q_L = l_{p_L}}} (|\bar{I}(p_L)-\bar{I}(q_L) | + E(q_L))\right)
\end{equation}
And the diffusion energy for border occlusions are found by by:
\begin{equation}
E(p_L) = \min_{l_{p_L}=\{0,\dots, l_{p_{Lf}}-2\}} \left( \dfrac{1}{2 | q_L \in \mathcal{N}(p_L) \wedge l_{q_L=l_{p_L}} |} \; \sum_{q_L \in \mathcal{N}(p_L) \wedge l_{q_L = l_{p_L}}} (|\bar{I}(p_L)-\bar{I}(q_L) | + E(q_L))\right)
\end{equation}
The diffusion energy will be calculated for each occluded point and for each point the disparity which corresponds the minimum $E(p_L)$ is set as the disparity $l_{p_L}$ for the occluded point.

\subsection{Weighted Least Squares (WLS)}
In this method, all the non-occluded and filled occluded neighbors in a neighborhood around the occluded point is considered valid points and is used as control points in interpolation.\\

Since the neighborhood contains both foreground points and background points and the occluded point is expected to be a part of the background then the background points should have higher influence than foreground points. It is assumed that the color intensity between objects is significantly different and this property can be used to distinguish between foreground points and background points. \\

Each error term in the aggregated residual should be weighted so the foreground doesn't have much influence. The aggregated residual is then defined as:
\begin{equation}
  \Delta = \sum_{q_L \in \mathcal{N}(p_L)} w_{q_L} (\hat{l}_{p_L}(p_L)-l_{p_L}(q_L))^2
\end{equation}
where $w_{q_L} = e^{-\mu_L | \bar{I}(p_L) - I(q_L)|}$ (the weight) is the likelihood of $p_L$ with $q_L$ under the assumption of an exponential distribution model of $| \bar{I}_(p_L)- I(q_L) |$. $\bar{I}(p_L)$ is the mean intensity of $p_L$ and $\mu_L$ and is also called the decay rate. $\hat{l}_{p_L}(p_L)$ is the estimated disparity of $p_L$ (will be estimated during interpolation) and $l_{p_L}(q_L)$ is the disparity of $q_L$. \\

$\bar{I}(p_L)$ is the mean intensity of $p_L$ which can be obtained using mean shift algorithm in a window around $p_L$. To estimate this value then initialize the algorithm with $\bar{I}(p_L) $ equal to the intensity of $p_L$ then the mean shift algorithm repeatedly picks those neighbors inside the window that satisfy $| \bar{I}(p_L) - I (q_L) | \geq 3\mu^{-1}$ and the assign the average of intensities of the selected neighbors to $\bar{I}(p_L)$ until $\bar{I}(p_L)$ converges to a fixed average. $|\bar{I}(p_L) - I(q_L)|$ has decay rate $\mu_L$ which is related to the decay rate $\mu$ of the variable $|I(p_L) - I(q_L)|$ by $\mu_L^2 = \mu$.\\

For this model a matrix containing the coordinates for all the control points is generated:
\begin{equation}
F = \begin{bmatrix}
  x_1 & y_1 & 1 \\
  \vdots & \ddots & \vdots\\
  x_n & y_n & 1
\end{bmatrix}
\end{equation}
and a vector with the corresponding labels for the coordinates in $F$ is generated:
\begin{equation}
L = [l_1 \; \cdots \; l_N]
\end{equation}
Then a Linear model can be expressed as:
\begin{equation}
l_{p_L} = a + b x (p_L) + c y (p_L)
\end{equation}
Where $(x(p_L),y(p_L))$ is the coordinates of $p_L$ and $a$, $b$ and $c$ are the model parameters. \\

The weights for the control points can be express in a vector as:
\begin{equation}
w = [w_{q_{L1}} \; w_{q_{L2}} \; \cdots \; w_{q_{LN}}]'
\end{equation}
Then we compute two new matrices, $F_w$ and $L_w$:
\begin{flalign}
&& F_w &= diag(w)F && \\
&& L_w &= diga(w)L &&
\end{flalign}
The model parameter vector:
\begin{equation}\label{eq:parvec}
P = [\, a \; b \; c \,]'
\end{equation}
By combining the equations above then the following equation is given:
\begin{equation}
P = (F^T_wF_w)^{-1}F^T_wL_w
\end{equation}
With these equation the disparity of the occluded point can be estimated:
\begin{equation}
\hat{l}_{p_L} = [1 \; x(p_L) \; y(p_L)] P
\end{equation}

\subsection{Segmentation-based Least Squares (SLS)}\label{sec:sls}
The biggest difference between WLS and SLS is that SLS only uses non-occluded points as control points. The control points is a subset of the non-occluded neighboring points. The control points are segmented from the neighborhood by applying different constraints: visibility constraint, disparity gradient constraint, and color similarity cues. This description of the method will in this report be simple. For a more thoroughly description refer to \cite{huq2013occlusion}. \\

First find occluded points which have at least one non-occluded neighboring point. Then these points are sorted by a priority which is found by checking the homogeneity i.e color similarity between the occluded point, $p_L$ and the neighboring non-occluded points. In this case, equation \ref{eq:inverthomo} can be used to find the homogeneity. The lower result given by the equation the higher the homogeneity is.
\begin{flalign}
&& \sum_{q_L \in N(p_L)} &= \psi (p_L,q_L) && \label{eq:inverthomo}
\end{flalign}
Where $q_L$ is a non-occluded point, $N(p_L)$ is a set of neighboring non-occluded points and $\psi (p_L,q_L)$ is a cut-off function expressed as:
\begin{flalign}
&& \psi(p_L,q_L) &= \begin{cases}
  |\bar{I}(p_L) - I(q_L) | &, \text{ if} |\bar{I}(p_L) - I(q_L) |  \leq 3 \mu^{-1}_L \\
  3 \mu^{-1}_L &, \text{ otherwise}
\end{cases}
&& 
\end{flalign} 
When the occluded point with highest priority is found then some initial control points from $N(p_L)$ are needed. First only points from the background is wanted therefore only points in $N(p_L)$ which satisfies $l_{q_L} < l_{p_{Lf}}$ where $l_{q_L}$ is the disparity of a point, $q_L$ in $N(p_L)$ and $l_{p_{Lf}}$ is the disparity of the nearest point from the foreground which is expected to be the nearest non-occluded point to the right of $p_L$. But a narrow object in the scene might result in the nearest non-occluded point to the right to be a part of the background therefore a second constraint is added. The nearest non-occluded point to the left is assumed to be a part of the background and therefore all points in $N(p_L)$ which satisfies $|l_{p_{Lb}} - l_{q_L}| \leq 1$ should also be part of the background. $l_{p_{Lb}}$ is the disparity of the nearest non-occluded point to the left. With these constraints a combined constraint can be expressed:
\begin{flalign}
 |l_{p_{Lb}} - l_{q_L}| \leq 1 \vee l_{q_L} < l_{p_{Lf}}
\end{flalign}

All points in $N(p_L)$, which satisfies this combined constraint, is assumed to be part of the background but might contain points from multiple background surfaces. It is assumed, from looking at the data sets from \cite{middlebury2016}, that $N(p_L)$ only contains points from either one background surface or two background surfaces. \\

First it is needed to calculated whether the points in the updated $N(p_L)$ belongs to one or two surfaces and for this the $l_{max} - l_{min} \leq 1$ can be used. $l_{max}$ is the maximum disparity in $N(p_L)$ and $l_{min}$ is the minimum disparity and if these disparities is close it is assumed that they belong to the same surface and all points in $N(p_L)$ can be used as controls points for finding the disparity for $p_L$. If $l_{max} - l_{min} > 1$ then $N(p_L)$ should be divided into two sets, $N_1(p_L)$ and $N_2(p_L)$. \\
If a point $q_L$ in $N(p_L)$ satisfies $|l_{max} - l_{q_L}| \leq 1$ then it belongs to $N_1(p_L)$ or if it satisfies $|l_{min} - l_{q_L}| \leq 1$ then it belongs to $N_2(p_L)$. With $N(p_L)$ divided into two groups it is needed to determine which set $p_L$ belongs to. For this the average truncated color distance is used and it is expressed as:
\begin{flalign}
  && D(p_L, N_i(p_L)) &= \frac{1}{|N_i(p_L)|} \sum_{q_L \in N(p_L)} \psi(p_L,q_L) &&
\end{flalign}
If $D(p_L, N_1(p_L)) < D(p_L, N_2(p_L))$ then $p_L$ belongs to $N_1(p_L)$ else it belongs to $N_2(p_L)$. Then the set, which $p_L$, belongs to can used as control points for finding the disparity of $p_L$.\\

\section{Occlusions filling result}
In \cite{huq2013occlusion} it is found that SLS gives the best result followed by DIS, NDA, and WLS but the runtime SLS is slower than NDA and WLS and faster than DIS. Since the project focuses on implementing a fast stereo algorithm and focuses more on the stereo algorithm itself, NDA is chosen as the method to fill occlusions since it is the better performing of the two faster methods. 

\section{Mini-conclusion}
To conclude this chapter the findings for each area in stereo vision will be examined and it will be specified which solution will be used from this point.
\subsection*{Rectification of Images}
This project will not delve further into the subject of rectification and data sets from \cite{middlebury2016}, which already is rectified, will be used in this project.

\subsection*{Color Space}
The result from \cite{chambon2005colour} shows that color spaces in most cases is a bit better than using grayscale images but the run time is much higher. This result depends on the algorithm used therefore when an algorithm have been chosen a test should check if grayscale images perform much worse than using color images and what the impact to run time is.

\subsection*{Resolution and Disparity Precision}
To have a disparity precision of 2 mm at 1500 mm using \textit{Imaging Source DMK 72BUC02} cameras with a resolution of 2592$\times$1944 and a pixel size of 2.2 \si{\micro\meter} a lens with a focal length of 10 mm should be used together with subpixel refinement.

\subsection*{Occlusions}
4 different methods to fill occlusions where described and from these NDA is chosen since it is the better performing of the two faster algorithms.