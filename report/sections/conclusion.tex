\chapter{Conclusion}\label{ch:conclusion}
This chapter concludes on the project. In chapter \vref{ch:introduction} it was specified that the goal of this project was to design and implement a stereo vision algorithm and some questions were to be researched:
\begin{itemize}
  \item What obstacles occur within stereo vision?
  \item Which stereo vision algorithm exist, both being computationally efficient and at the same time providing good vision results?
  \item How can an architecture be designed and optimized for executing a stereo vision algorithm?
\end{itemize}
The findings for each question will be discussed for each question below.
In the end of chapter \vref{ch:introduction} the A$^3$ model were described and in chapter \vref{ch:designmet} the Gajski-Kuhn Y-chart were described. These models were used for structuring the report and project. In the end of this chapter the use of these models will be discussed.

\section*{1 What obstacles occur within stereo vision?}
In chapter \vref{ch:appanalysis} the basics of stereo vision were researched along with some challenges stereo vision brings along with it. With occlusions being a significant subject when discussing stereo vision, occlusions and the different types of occlusions were discussed and different methods to find and fill these occlusions were researched. From this research, it was found that some methods for filling occlusions exists. Four methods were looked at in \cite{huq2013occlusion}. These methods where neighbor's disparity assignment, diffusion in intensity space, segmentation-based least square and weighted least square. The results from that article were used to conclude that a simple occlusion filling method (neighbor's disparity assignment) would be used for this project since the focus is on execution time and the improvement in matching was not high enough for neglect the runtime of the simpler method.\\
Another subject researched in chapter \vref{ch:appanalysis} was the depth precision. HSA Systems requires a high depth precision for future assignments. HSA Systems specified a sensor they wanted to use for the stereo vision system and it was calculated whether the wanted precision where possible to achieve. It was found that the required depth precision was possible to achieve by sacrifice the vertical scene size and using sub-pixel refinement.\\
Other subjects researched where rectification of images and the impact from color spaces. The challenges with rectification were described but for this project, it was not researched further since the test images used are already rectified. For the subject of color spaces the foundings in \cite{chambon2005colour} were used. From this, it was concluded to use grayscale images if the impact on stereo matching quality were not too significant since grayscale images can speed up the runtime but will always have worse stereo matching quality.\\

\section*{2 Which stereo vision algorithm exist, both being computationally efficient and at the same time providing good vision results?}
There exists a lot of stereo vision algorithms but HSA Systems reduced the search to algorithms with a focus on edge preserving algorithms to ensure a good distinction between objects. Two efficient algorithms which preserve the edge were found and these were: Efficient Edge Preserving Stereo Matching and Fast Cost-Volume. In chapter \ref{ch:appanalysis} the algorithms were described and then an algorithm were chosen to be used in this project. For choosing an algorithm both algorithms were simulated in Python and the run-time of each algorithm and the quality of the resulting disparity map were compared. To ensure that the programming skills of the author don't affect the choice of algorithm to use further in the project the theoretical computational complexity of each algorithm were calculated. Both the simulation and the theoretical complexity calculations show that the Efficient Edge Preserving Stereo Matching algorithm is better than the Fast Cost-Volume both when comparing the computational complexity and the quality of the resulting disparity map. Efficient Edge Preserving Stereo Matching was chosen due to the results of the comparison.

\section*{3 How can an architecture be designed and optimized for executing a stereo vision algorithm?}
With an algorithm chosen, the design process towards an architecture could begin. The platform given by HSA Systems was a Zedboard which contains a Zynq Z7020 SoC. The Zynq Z7020 SoC contains both a general purpose processor and an FPGA. This project only focuses on an FPGA implementation hence the GPP part of the platform were not used since this was stated in the project description from HSA Systems. The GPP part was reserved for HSA Systems in case they wanted to use it for other applications in their products.\\

A final FPGA implementation was not achieved in this project but the steps towards an implementation such as scheduling and allocation were described in \ref{ch:archdesign}. It is concluded the architecture design and optimization process for the EEPSM algorithm can follow the general procedure for FPGA design. In the design process, we went through in this project some challenges emerged: the implementation of exponential function and memory usage.\\

The EEPSM algorithm uses exponential function and this function isn't trivial to implement. Different estimates and approximations were looked at: using a power series, using CORDIC and using a lookup table. The power series implementation was unfeasible since it required too many terms and too large constants. Among the CORDIC implementation and the Lookup table implementation, it was found that the lookup table would fit this project the best. This is due to CORDIC being an iterative algorithm and hence will result in higher runtime while lookup table only required an acceptable amount of logic elements since the quantity of numbers used in the exponential function is limited.\\

Another challenge which emerged was a significant memory usage. The algorithm requires saving a lot of copies of the cost images due to the aggregation step. This resulted in the algorithm to require above the available memory on the platform. The algorithm was changed to reduce the memory usage. Two different alterations were considered: Cutting the images into sub-images or down-sampling using image pyramids. The alteration was compared and it were found that the down-sampling altered the resulting disparity map too much to be acceptable while the sub-image alteration didn't affect the algorithm much and hence it were chosen to cut the original images into sub-images.\\

It can be concluded that to implement a stereo vision algorithm using large image sizes then memory usage can be a challenge and have to be optimized. The large images required for a high depth precision also introduces a high disparity range. \\

If the project were to be recreated we would ask to change the project description to include hardware/software co-design implementation instead of only a hardware implementation. This would require learning new theory about hardware/software co-design and use other design models such as the Rugby model \cite{jantsch1999rugby}. The ARM$^\text{\textregistered}$ Processing system contains a NEON engine which is a general-purpose SIMD engine \cite{neon}. The NEON engine works with its own pipeline, register and execution hardware hence it can execute operations parallel with the GPP. The instruction set for the NEON engine include instructions such as MAC by vector, vector minimization, vector multiplication etc. These operations can be multiple parts of the algorithm such as aggregation, SAD, and minimization when finding the disparity value. The inclusion of a GPP can also simplify the memory management. Large images and high disparity ranges require a lot of memory management. The GPP could handle the memory management by controlling pointers to pixels which can be sent to architecture in the FPGA.

\section*{Evaluation of Design Models}
To structure the report the A$^3$ model were used. This model works in three domains and helps limit focus in each domain. i.e. in the first part of report only focus on the application, the second part focus on the algorithm and the last part focus on the architecture.\\
The limiting of focus area can help simplify parts of the design process and limit some of the workload i.e. when researching solutions for occlusion filling the details of implementation in architecture is not considered. This helps to limit the details needed to be considered.\\

To structure the design process of a hardware architecture the Gajski-Kuhn Y-chart were used. This model can help to limit the design process into different domains and abstraction levels. The journey around in the model can be done in different ways i.e. starting at low abstraction levels and moving up in abstraction levels or start at high abstraction levels and moving towards lower levels. It was chosen to follow an FPGA methodology where the design process starts at highest abstraction levels when you have reached a wanted level of abstraction the FPGA software is used to synthesize the design and giving a final implementation. \\

The models helped to structure the project work and in chapter \vref{ch:archdesign} at some occasion examples of information in later domains in the A$^3$ model requires the design process to backtrack into earlier domains to solve challenges. \\

The Gajski-Kuhn Y-chart were feasible for this project since it focuses on a hardware design but if the GPP part of the Zynq SoC were to be used then the Y-chart would be insufficient since the segregation between hardware and software is not naturally modeled in the Y-chart.\\

